\documentclass[17pt]{extarticle}
\usepackage[margin=0.75in]{geometry}
\usepackage{fontspec}
\usepackage{parskip}
\usepackage{titlesec}

% Large slide number headers
\titleformat{\section}{\normalfont\LARGE\bfseries}{}{0em}{}
\titlespacing*{\section}{0pt}{0pt}{0.5em}

% Each section on its own page
\newcommand{\slide}[1]{%
  \clearpage
  \section*{#1}
  \vspace{0.5em}
}

\begin{document}

\slide{Slide 1: Cognitive MRI of AI Conversations}

``How many conversations have you had with ChatGPT? Hundreds? Thousands? All of them sitting there, buried in a timeline.''

``What if there's structure hiding in there?''

``We call this a `Cognitive MRI'---extracting a knowledge map from your chat logs.''

\slide{Slide 2: Why Now? The Scale of the Opportunity}

``ChatGPT has about a billion users. That's a lot of conversations.''

``And these logs capture something different. Citation networks give you papers---outputs. Social networks give you connections.''

``But chat logs? They capture the process. How ideas develop. The back-and-forth.''

``Today: one case study.''

\slide{Slide 3: The Big Picture: Externalized Cognition}

``There's a concept called Distributed Cognition. The idea is simple: thinking doesn't just happen in your head. It happens between you and your tools.''

``When you talk to an LLM, you're thinking out loud. Offloading work. And ideas get built through that conversation---not just looked up.''

``Traditionally, what gets archived? The product. The final draft. Clean and polished.''

``The messy part---the exploring, the dead ends---that usually disappears.''

``Think about fixing a bug. Twenty rounds of trying things. False starts, backtracking. That's where you actually learn how the system works. But the commit message? One line: `fixed bug.' The process disappears.''

``That's cognitive dark matter. And chat logs might capture it.''

``So here's what we did.''

\slide{Slide 4: From Chat Logs to Network}

``So how do we turn a linear chat log into a network?''

``Start with a chat history. It's just a timeline. January, you're debugging a Python error. February, a banana bread recipe. March, back to debugging.''

``First, we turn each conversation into a vector---an embedding. You can use classical approaches like TF-IDF, but we use LLM-based embeddings. They're denser, more semantically meaningful. Similar topics land close together in that space.''

``So now each conversation is a node. To form edges, we measure cosine similarity between embeddings---how close are two conversations in meaning? Then we apply a threshold, theta. If the similarity is above theta, we draw an edge. If not, no connection.''

``What happens? Those two coding chats---January and March---snap together. Banana bread floats off on its own.''

``The point is: close in time doesn't mean close in thought. The network reconnects what the timeline splits apart. How we choose theta? That comes later.''

\slide{Slide 5: Whose Voice Matters?}

``There's a problem though. AI responses are wordy. Lots of filler. If you treat everything the same, the AI's voice could drown out yours.''

``So we split user messages from AI messages. Intuitively, your prompts carry the intent---what you actually care about. The AI just adds context.''

``We introduce a weighting parameter, alpha. How much should we prioritize the user? Should we even prioritize the user at all? Maybe the AI's responses actually categorize topics better?''

``That's an empirical question. Let's test it.''

\slide{Slide 6: Rigorous Parameter Tuning: 2D Ablation Study}

``We tested 63 configurations to answer two questions. What threshold theta? And does user weighting actually help?''

``For theta, there's a tipping point around 0.875. Below that, everything connects---it's a hairball. We chose 0.9, just past the transition.''

``For alpha---remember our intuition that the user's voice should matter more? The data confirms it. Modularity peaks precisely at 2-to-1. Not 1-to-1, not 3-to-1. Exactly 2-to-1.''

``So the ablation study does two things: confirms our intuition, and finds the optimal ratio. Final result: Q equals 0.750.''

``These choices aren't arbitrary. They're data-driven.''

\slide{Slide 7: The Cognitive MRI: 15 Knowledge Domains}

\emph{[Pause---let them look]}

``Hundreds of nodes, thousands of edges. Modularity 0.75. 15 communities---15 knowledge domains.''

``I labeled these communities manually by inspecting each one. Though I did try having an LLM automate it, and it worked reasonably well.''

``Let me walk you through the major domains. Up here, my master's thesis work in pure math. Next to it, stats and probability. Over here, ML and AI. Philosophy and alignment. Down here, coding projects---more spread out, different projects in their own clusters.''

``The communities were discovered by the algorithm. I just interpreted what they mean.''

\slide{Slide 8: Insight 1: Structural Heterogeneity}

``Different topics have different shapes.''

``Theory---math, ML concepts, philosophy---clusters tightly. Clustering coefficient around 0.58. That's high. Small-world structure.''

``That makes sense. Theory means revisiting core ideas. Refining definitions. Everything links back.''

``Coding is looser. Around 0.39. More like a tree.''

``You fix one bug, move on. Less backtracking. Projects stay in their own lanes. Metaprogramming here, physics simulation there. Not much crossover.''

``This is suggestive. We'd need more users to know if it holds up.''

\slide{Slide 9: Insight 2: A Taxonomy of Bridges}

``We also looked at the connectors---the high-betweenness nodes that link different clusters.''

``Three patterns showed up.''

``Evolutionary bridges. Conversations that drift. See `Geometric Mean' here---the title sounds narrow, right? But the conversation sprawls. Started in stats and probability, drifted through ML and AI, ended up in practical programming. The title only captures where it began.''

``Integrative bridges. Deliberate synthesis. `mcts-code' here---Monte Carlo Tree Search implementation. It explicitly combines ML and AI concepts with practical programming. You're consciously building connections between theory and practice.''

``Pure bridges---rare shortcuts. `cuda-program-linux'---a single Linux config question that happens to link my physics simulation with practical programming. Rare, but powerful.''

``We're proposing this as a taxonomy. Based on what we saw.''

\slide{Slide 10: The Vision: Personal Knowledge Cartography}

\emph{[30 seconds MAX - don't linger]}

``Why does this matter? Right now, your chat history is an infinite scroll. Buried. The map makes it navigable.''

``Imagine: `Show me everywhere I discussed entropy.' The network lights up---connections you forgot about.''

\emph{[Move on to conclusion]}

\slide{Slide 11: Cognitive MRI: A Proof of Concept}

``So what did we show? That LLM conversation logs can be transformed into meaningful cognitive maps. The structure is real---not noise.''

``What were the key ideas? User-weighted embeddings---2-to-1 ratio---to capture your intent, not just the AI's responses. Tuned link thresholds to optimize modularity. We found heterogeneous topology---theoretical domains form hubs, practical domains form trees. We proposed a taxonomy of bridge types. And we ran a 2D ablation study showing these choices weren't arbitrary.''

``But let's be honest about the limitations. This is N-of-1---one user, one platform, one snapshot in time. No ground truth to validate against. We don't know yet if these patterns generalize.''

``Where do we go from here? More users. Track how these networks evolve over time. User studies to see if the map actually helps people find things.''

``If you have access to larger datasets, we'd love to collaborate.''

``Thanks. Happy to take questions.''

\end{document}
